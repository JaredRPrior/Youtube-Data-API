{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key='AIzaSyDbQTp-qKJP55kAAEgXP2vD80uHK4fVG-s'\n",
    "#api_key='AIzaSyAQBq0X5Q3JCwwAlqx7hP24x0tS6NYpZpE'\n",
    "#api_key = 'AIzaSyDBAlVzk0Q_pqFqdxdosJ09AjS9RhN1o28'\n",
    "#api_key = 'AIzaSyB2VjIO1qBKtHIYfM1kLGK0X4huo5cPgJg'\n",
    "#api_key = \"AIzaSyCGEBtZPu5Bpfa-wJIMcE6QPnzIYiNwD5k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use an API key to get access to the Youtube Data API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = build(\"youtube\",'v3',developerKey=api_key) # establishes a connection with the Youtube Data API v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we craft a function to retrieve comments from any given \n",
    "video by its video ID. This function returns all of the comments in a \n",
    "dictionary, where the comment authors are keys. Each of their comments\n",
    "and their number of likes are stored as the values in a list of \n",
    "heterogenous lists. The video statistics are also retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(video_id):\n",
    "    authors = {}\n",
    "    def exists(author_id):\n",
    "        return author_id in authors\n",
    "    stats = yt.videos().list(id=video_id,part=\"statistics\").execute()['items'][0]['statistics']\n",
    "    next_page = None\n",
    "    pahe = 0\n",
    "    while True:\n",
    "        res = yt.commentThreads().list(videoId=video_id,\n",
    "                                       part=\"snippet,replies\",maxResults=100,pageToken = next_page).execute()\n",
    "\n",
    "        for comment_data in res['items']:\n",
    "            comment_data = comment_data['snippet']['topLevelComment']['snippet']\n",
    "            comment = comment_data['textOriginal']\n",
    "            likes = comment_data['likeCount']\n",
    "            author = comment_data['authorDisplayName']\n",
    "            if exists(author):\n",
    "                authors[author].append([comment,likes])\n",
    "            else:\n",
    "                authors[author] = []\n",
    "                authors[author].append([comment,likes])\n",
    "        next_page = res.get('nextPageToken')\n",
    "        if next_page==None or pa:\n",
    "            break\n",
    "    return authors, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function retrieves all of a given channel's Youtube\n",
    "videos and stores the video's video Id, title, date of publishing,\n",
    "its description, and its statistics. It uses the get_video_comments method to \n",
    "create a dictionary of all the comments, and it stores all of these\n",
    "attributes as a heterogenous list within a list of other video-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_videos(channel_id):\n",
    "    res = yt.channels().list(id=channel_id,\n",
    "                              part='contentDetails').execute()\n",
    "    items = res['items']\n",
    "    playlist_id = items[0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    pages = 10\n",
    "    videos = []\n",
    "    while True:\n",
    "        try:\n",
    "            result = yt.playlistItems().list(playlistId=playlist_id, \n",
    "                                               part='snippet',\n",
    "                                               maxResults=50,\n",
    "                                               pageToken = next_page).execute()\n",
    "            for res in result['items']:\n",
    "                video_id = res['snippet']['resourceId']['videoId']\n",
    "                video_title = res['snippet']['title']\n",
    "                video_publish_date = res['snippet']['publishedAt']\n",
    "                video_description = res['snippet']['description']\n",
    "                try:\n",
    "                    authors, stats = get_video_comments(video_id)\n",
    "                    videos.append([video_title, video_publish_date, video_description, stats, authors])\n",
    "                except Exception:\n",
    "                    print(\"Comments disabled\") \n",
    "            next_page = result.get('nextPageToken')\n",
    "            page += 1\n",
    "            print(page)\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "            break\n",
    "        if next_page == None or page > pages:\n",
    "            break\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some channels might appear as a \"user\" instead of a \"channel,\" so we convert those by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_id = yt.channels().list(part=\"id\",forUsername='CNN').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_id = yt.channels().list(part=\"id\",forUsername='FoxNewsChannel').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "Comments disabled\n",
      "1\n",
      "<class 'Exception'>\n"
     ]
    }
   ],
   "source": [
    "cnn_data = channel_videos(cnn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_data = channel_videos(fox_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process our data and organize it so that \n",
    "it may be used for analysis. We will also cache our data\n",
    "at this point, since Google restricts the number of requests\n",
    "we can make with the API and we want to make the most of\n",
    "every request we are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_channel_data(data, channel):\n",
    "    channel_file = open(channel,\"w+\")\n",
    "    new_line = \"\\n\\n\"\n",
    "    for video in data:\n",
    "        # video_title, video_publish_date, video_description, stats, authors\n",
    "        video_title, video_publish_date,\\\n",
    "        video_description, stats, authors = video\n",
    "        vid_doc = video_title+\"\\t\"+video_publish_date+\"\\t\"+\\\n",
    "        video_description+str(stats)+new_line\n",
    "        channel_file.write(vid_doc)\n",
    "        for author in authors:\n",
    "            for comment in authors[author]:\n",
    "                text = comment[0]\n",
    "                likes = comment[1]\n",
    "                comment_doc = author+\" said: \"+\\\n",
    "                text+\"(\"+str(likes)+\" likes)\\n\"\n",
    "                channel_file.write(comment_doc)\n",
    "        channel_file.write(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(cnn_data, \"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fox_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7fcadbcf5566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcache_channel_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfox_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fox\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fox_data' is not defined"
     ]
    }
   ],
   "source": [
    "cache_channel_data(fox_data, \"Fox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data cached, we can now begin to look at some basic trends. We'll do some analysis on the video statistics and plot the like/dislike ratios for videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the necessary libraries to do plotting and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from matplotlib.dates import WeekdayLocator\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,rrulewrapper, RRuleLocator, drange)\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write some helper functions to take care of the details. First, a function to convert the raw dates from our list of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_date(date):\n",
    "    date_raw = date.split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_raw[0])\n",
    "    month = int(date_raw[1])\n",
    "    day = int(date_raw[2])\n",
    "    return year, month, day\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write a function to process the statistics for each video and return the number of views, likes, dislikes, and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_stats(video):\n",
    "    title, publish_date, description, stats, authors = video\n",
    "    year, month, day = convert_raw_date(publish_date)\n",
    "    date = datetime.date(year,month,day)\n",
    "    views = stats['viewCount']\n",
    "    likes = stats['likeCount']\n",
    "    dislikes = stats['dislikeCount']\n",
    "    num_comments = stats['commentCount']\n",
    "    return title, date, description, int(views), int(likes), int(dislikes), int(num_comments), authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our processing function to build up lists of different values that we can plot against the publishing dates of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(videos):\n",
    "    like_nums = []\n",
    "    dislike_nums = []\n",
    "    comment_nums = []\n",
    "    like_dislike_ratios = []\n",
    "    dates = []\n",
    "    titles = []\n",
    "    comments = []\n",
    "    view_nums = []\n",
    "    for video in videos:\n",
    "        title, date, description, views, likes, dislikes, num_comments, authors=process_video_stats(video)\n",
    "        like_nums.append(likes)\n",
    "        dislike_nums.append(dislikes)\n",
    "        like_dislike_ratios.append(likes/dislikes)\n",
    "        comment_nums.append(num_comments)\n",
    "        dates.append(date)\n",
    "        titles.append(title)\n",
    "        comments.append(authors)\n",
    "        view_nums.append(views)\n",
    "    return like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates, titles, comments, view_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cnn_data = process_videos(cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fox_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-34c04cda5b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessed_fox_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfox_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fox_data' is not defined"
     ]
    }
   ],
   "source": [
    "processed_fox_data = process_videos(fox_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now write a plotting function to plot based on the metrics we've produced from our processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, dates, title):\n",
    "    # tick every 5th easter\n",
    "    mpl.rcParams['figure.dpi'] = 150\n",
    "    loc = WeekdayLocator(byweekday=MO, interval=1)\n",
    "    formatter = DateFormatter('%Y-%m-%d')\n",
    "    fig, ax = plt.subplots()\n",
    "    y_mean = statistics.mean(data)\n",
    "    plt.plot_date(dates, data)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_tick_params(rotation=30, labelsize=10)\n",
    "    normalized_dates = []\n",
    "    for i in dates:\n",
    "        normalized_dates.append(to_integer(i))\n",
    "    z = np.polyfit(normalized_dates, data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.axhline(y=y_mean, color='r', linestyle='-',label='Mean Average: {}'.format(int(y_mean)))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to plot video statistics over time from any given channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[0],processed_cnn_data[4], \"CNN Video Likes\")\n",
    "plot_data(processed_fox_data[0],processed_fox_data[4], \"FOX News Video Likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[1],processed_cnn_data[4], \"CNN Video Disikes\")\n",
    "plot_data(processed_fox_data[1],processed_fox_data[4], \"FOX News Video Disikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[3],processed_cnn_data[4], \"CNN Video Likes/Disikes\")\n",
    "plot_data(processed_fox_data[3],processed_fox_data[4], \"FOX News Video Likes/Disikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[7],processed_cnn_data[4], \"CNN Video Views\")\n",
    "plot_data(processed_fox_data[7],processed_fox_data[4], \"FOX News Video Views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[2],processed_cnn_data[4], \"CNN Video Comments\")\n",
    "plot_data(processed_fox_data[2],processed_fox_data[4], \"FOX News Video Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
