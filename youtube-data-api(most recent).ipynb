{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we lay out some methods in order to treat our API keys as though they were a revolving door, only spinning the door when we run into a timeout from Google. The point of this revolving door is to circumvent the limitations of Google's API quotas in real-time to vastly improve the number of requests we can make in one execution of our program. access to the Youtube Data API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient import errors\n",
    "import ast\n",
    "import json\n",
    "# Author: Jared Prior\n",
    "\n",
    "# API Revolving Door \n",
    "# intercepts rejected API requests and spins \n",
    "# up a new Youtube build using the next API\n",
    "# key in the pool. The rejected key is discarded\n",
    "# and not considered again, as the revolving\n",
    "# door only dispenses each API key from the \n",
    "# pool once.\n",
    "current_key = 0\n",
    "api_revolving_door = [\"AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk\",\n",
    "                      \"AIzaSyAXGe4C55z3hL8Znu2vKkQzjUaV0HgWfhs\", \n",
    "                      \"AIzaSyCPj4BPl3XcqR8L1GMEblZKKJds3hVIKm8\", \n",
    "                      \"AIzaSyACcJRNByz_GLpvZGrdl5RjCtJiH2UGDbo\"]\n",
    "def yt_build(key):\n",
    "    # builds Youtube object from API key\n",
    "    yt = build(\"youtube\",'v3',developerKey=key) # establishes a connection with the Youtube Data API v3\n",
    "    return yt\n",
    "def door_spin():\n",
    "    global current_key\n",
    "    # retrieves the next build or flags empty pool\n",
    "    if current_key > (len(api_revolving_door)-2):\n",
    "        return None\n",
    "    else:\n",
    "        current_key += 1\n",
    "    api_key = api_revolving_door[current_key]\n",
    "    yt = yt_build(api_key)\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = yt_build(api_revolving_door[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we craft a function to retrieve comments from any given \n",
    "video by its video ID. This function returns all of the comments in a \n",
    "dictionary, where the comment authors are keys. Each of their comments\n",
    "and their number of likes are stored as the values in a heterogenous list. The video statistics are also retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_reason(err):\n",
    "    if err.resp.get('content-type', '').startswith('application/json'):\n",
    "        reason = json.loads(err.content).get('error').get('errors')[0].get('reason')\n",
    "    else:\n",
    "        reason=\"Unexplained\"\n",
    "    return reason\n",
    "def get_video_comments(video_id, yt):\n",
    "    # retrieves comments from a given Youtube video by its ID\n",
    "    # returns a dictionary of authors and their comments\n",
    "    global current_key\n",
    "    authors = {}\n",
    "    def exists(author_id):\n",
    "        return author_id in authors\n",
    "    stats = yt.videos().list(id=video_id,part=\"statistics\").execute()['items'][0]['statistics']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    pages = 5\n",
    "    while True:\n",
    "        res = yt.commentThreads().list(videoId=video_id,\n",
    "                                       part=\"snippet,replies\",maxResults=100,pageToken = next_page).execute()\n",
    "\n",
    "        for comment_data in res['items']:\n",
    "            comment_data = comment_data['snippet']['topLevelComment']['snippet']\n",
    "            comment = comment_data['textOriginal']\n",
    "            likes = comment_data['likeCount']\n",
    "            author = comment_data['authorDisplayName']\n",
    "            if exists(author):\n",
    "                authors[author].append([comment,likes])\n",
    "            else:\n",
    "                authors[author] = []\n",
    "                authors[author].append([comment,likes])\n",
    "        next_page = res.get('nextPageToken')\n",
    "        page +=1\n",
    "        if next_page==None or page > pages:\n",
    "            break\n",
    "    return authors, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function retrieves all of a given channel's Youtube\n",
    "videos and stores each video's video ID, title, date of publishing,\n",
    "its description, and its statistics. It uses the get_video_comments method to \n",
    "create a dictionary of all the comments, and it stores all of these\n",
    "attributes as a heterogenous list within a list of other video-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_videos(channel_id, yt):\n",
    "    # returns all the videos of a particular Youtube\n",
    "    # channel, and returns data for each video (title, description,\n",
    "    # comments, date, statistics, etc.)\n",
    "    global current_key\n",
    "    res = None\n",
    "    while res == None:\n",
    "        try:\n",
    "            res = yt.channels().list(id=channel_id,\n",
    "                                  part='contentDetails').execute()\n",
    "        except errors.HttpError as error:\n",
    "            if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                print(\"Disabled comments: %s\" % error)\n",
    "                pass\n",
    "            elif error.resp.status == 403:\n",
    "                print(\"Error1: %s\" % error)\n",
    "                yt = door_spin()\n",
    "                if yt == None:\n",
    "                    print(\"Breaking\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error2: %s\" % error)\n",
    "                pass\n",
    "    items = res['items']\n",
    "    playlist_id = items[0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    pages = 10\n",
    "    videos = []\n",
    "    while True:\n",
    "        print(current_key)\n",
    "        try:\n",
    "            result = yt.playlistItems().list(playlistId=playlist_id, \n",
    "                part='snippet',\n",
    "                maxResults=50,\n",
    "                pageToken = next_page).execute()\n",
    "            for res in result['items']:\n",
    "                video_id = res['snippet']['resourceId']['videoId']\n",
    "                video_title = res['snippet']['title']\n",
    "                video_publish_date = res['snippet']['publishedAt']\n",
    "                video_description = res['snippet']['description']\n",
    "                try:\n",
    "                    authors, stats = get_video_comments(video_id, yt)\n",
    "                    videos.append([video_title, video_publish_date, video_description, stats, authors])\n",
    "                except errors.HttpError as error:\n",
    "                    if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                        print(\"Disabled comments: %s\" % error)\n",
    "                        pass\n",
    "                    elif error.resp.status == 403:\n",
    "                        print(\"Error1: %s\" % error)\n",
    "                        yt = door_spin()\n",
    "                        if yt == None:\n",
    "                            print(\"Breaking\")\n",
    "                            return None\n",
    "                    else:\n",
    "                        print(\"Error2: %s\" % error)\n",
    "                        pass\n",
    "            next_page = result.get('nextPageToken')\n",
    "            page += 1\n",
    "            print(page)\n",
    "        except errors.HttpError as error:\n",
    "            if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                print(\"Disabled comments: %s\" % error)\n",
    "                pass\n",
    "            elif error.status == 403:\n",
    "                print(\"Error1: %s\" % error)\n",
    "                yt = door_spin()\n",
    "                if yt == None:\n",
    "                    print(\"Breaking\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error2: %s\" % error)\n",
    "                pass\n",
    "        if next_page == None or page > pages:\n",
    "            break\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some channels might appear as a \"user\" instead of a \"channel,\" so we convert those by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_id = yt.channels().list(part=\"id\",forUsername='CNN').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_id = yt.channels().list(part=\"id\",forUsername='FoxNewsChannel').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = channel_videos(cnn_id, yt) # loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "commentsDisabled\n",
      "Disabled comments: <HttpError 403 when requesting https://www.googleapis.com/youtube/v3/commentThreads?videoId=sG0cWeIry3I&part=snippet%2Creplies&maxResults=100&key=AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\">\n",
      "commentsDisabled\n",
      "Disabled comments: <HttpError 403 when requesting https://www.googleapis.com/youtube/v3/commentThreads?videoId=kXwSGCn_HUY&part=snippet%2Creplies&maxResults=100&key=AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\">\n"
     ]
    }
   ],
   "source": [
    "fox_data = channel_videos(fox_id, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process our data and organize it so that \n",
    "it may be used for analysis. We will also cache our data\n",
    "at this point, since Google restricts the number of requests\n",
    "we can make with the API and we want to make the most of\n",
    "every request we are allowed. We will include an unpackaging \n",
    "method so that we can retrieve the data from our text files\n",
    "in the same format as it is returned in the channel_videos\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_channel_data(data, channel):\n",
    "    channel_file = open(channel+\".txt\",\"w\")\n",
    "    new_line = \"\\n\\n\\n\"\n",
    "    for video in data:\n",
    "        # video_title, video_publish_date, video_description, stats, authors\n",
    "        video_title, video_publish_date, video_description, stats, authors = video\n",
    "        vid_doc = video_title+\"\\t\"+video_publish_date+\"\\t\"+\\\n",
    "        video_description+\"\\t\"+str(stats)+\"\\t\"\n",
    "        channel_file.write(vid_doc)\n",
    "        for author in authors:\n",
    "            for comment in authors[author]:\n",
    "                text = comment[0]\n",
    "                likes = comment[1]\n",
    "                comment_doc = author+\"<><>\"+\\\n",
    "                text+\"sekil\"+str(likes)+\"sekil\"\n",
    "                channel_file.write(comment_doc)\n",
    "                channel_file.write(\"cachecomment\")\n",
    "        channel_file.write(\"cachevideo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will include an unpackaging \n",
    "method so that we can retrieve the data from our text files\n",
    "in the same format as it is returned in the channel_videos\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_cached_data(channel):\n",
    "    channel_file = open(channel,\"r\")\n",
    "    videos = channel_file.read().lstrip().split(\"cachevideo\")\n",
    "    video_list = []\n",
    "    for video in videos:\n",
    "        d=video.split(\"\\t\")\n",
    "        video_title = d[0]\n",
    "        video_date = d[1]\n",
    "        video_description = d[2]\n",
    "        video_stats = ast.literal_eval(d[3])\n",
    "        authors = {}\n",
    "        def exists(key):\n",
    "            return key in authors\n",
    "        for comment_data in d[4].split(\"cachecomment\"):\n",
    "            data = comment_data.split(\"<><>\")\n",
    "            author = data[0]\n",
    "            comment = data[1].split(\"sekil\")[0]\n",
    "            likes = data[1].split(\"sekil\")[0]\n",
    "            if exists(author):\n",
    "                authors[author].append([comment,likes])\n",
    "            else:\n",
    "                authors[author] = []\n",
    "                authors[author].append([comment,likes])\n",
    "        video_list.append([video_title,video_date,video_description,video_stats,authors])\n",
    "    return video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(cnn_data, \"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(fox_data, \"Fox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data cached, we can now begin to look at some basic trends. We'll do some analysis on the video statistics and plot the like/dislike ratios for videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the necessary libraries to do plotting and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import datetime\n",
    "from matplotlib.dates import WeekdayLocator\n",
    "from matplotlib.dates import DayLocator\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,rrulewrapper, RRuleLocator, drange)\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write some helper functions to take care of the details. First, a function to convert the raw dates from our list of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_date(date):\n",
    "    date_raw = date.split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_raw[0])\n",
    "    month = int(date_raw[1])\n",
    "    day = int(date_raw[2])\n",
    "    return year, month, day\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write a function to process the statistics for each video and return the number of views, likes, dislikes, and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_stats(video):\n",
    "    title, publish_date, description, stats, authors = video\n",
    "    year, month, day = convert_raw_date(publish_date)\n",
    "    date = datetime.date(year,month,day)\n",
    "    views = stats['viewCount']\n",
    "    likes = stats['likeCount']\n",
    "    dislikes = stats['dislikeCount']\n",
    "    num_comments = stats['commentCount']\n",
    "    return title, date, description, int(views), int(likes), int(dislikes), int(num_comments), authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our processing function to build up lists of different values that we can plot against the publishing dates of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(videos):\n",
    "    like_nums = []\n",
    "    dislike_nums = []\n",
    "    comment_nums = []\n",
    "    like_dislike_ratios = []\n",
    "    dates = []\n",
    "    titles = []\n",
    "    comments = []\n",
    "    view_nums = []\n",
    "    interactions = []\n",
    "    descriptions = []\n",
    "    for video in videos:\n",
    "        title, date, description, views, likes, dislikes, num_comments, authors=process_video_stats(video)\n",
    "        like_nums.append(likes)\n",
    "        dislike_nums.append(dislikes)\n",
    "        like_dislike_ratios.append(dislikes/(likes+dislikes))\n",
    "        comment_nums.append(num_comments)\n",
    "        dates.append(date)\n",
    "        titles.append(title)\n",
    "        comments.append(authors)\n",
    "        view_nums.append(views)\n",
    "        interactions.append(num_comments+likes+dislikes)\n",
    "        descriptions.append(description)\n",
    "    return like_nums, dislike_nums, comment_nums, like_dislike_ratios,\\\n",
    "    dates, titles, comments, view_nums, descriptions, interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cnn_data = process_videos(cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_fox_data = process_videos(fox_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now write a plotting function to plot based on the metrics we've produced from our processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, dates, title):\n",
    "    # tick every 5th easter\n",
    "    mpl.rcParams['figure.dpi'] = 150\n",
    "    loc = DayLocator(interval=3)\n",
    "    formatter = DateFormatter('%Y-%m-%d')\n",
    "    fig, ax = plt.subplots()\n",
    "    y_mean = statistics.mean(data)\n",
    "    st=\"{:.3f}\".format(y_mean)\n",
    "    print(st)\n",
    "    plt.plot_date(np.array(dates), np.array(data))\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_tick_params(rotation=30, labelsize=10)\n",
    "    normalized_dates = []\n",
    "    for i in dates:\n",
    "        normalized_dates.append(to_integer(i))\n",
    "    z = np.polyfit(normalized_dates, data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.axhline(y=y_mean, color='r', linestyle='-',label='Mean Average: {}'.format(float(st)))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,5))\n",
    "    style.use('fivethirtyeight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to plot video statistics over time from any given channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[0],processed_cnn_data[4], \"CNN Video Likes\")\n",
    "plot_data(processed_fox_data[0],processed_fox_data[4], \"FOX News Video Likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[1],processed_cnn_data[4], \"CNN Video Dislikes\")\n",
    "plot_data(processed_fox_data[1],processed_fox_data[4], \"FOX News Video Dislikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[3],processed_cnn_data[4], \"CNN Video Dislikes/Total\")\n",
    "plot_data(processed_fox_data[3],processed_fox_data[4], \"FOX News Video Dislikes/Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[7],processed_cnn_data[4], \"CNN Video Views\")\n",
    "plot_data(processed_fox_data[7],processed_fox_data[4], \"FOX News Video Views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[2],processed_cnn_data[4], \"CNN Video Comments\")\n",
    "plot_data(processed_fox_data[2],processed_fox_data[4], \"FOX News Video Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[9],processed_cnn_data[4], \"CNN Video Interactions\")\n",
    "plot_data(processed_fox_data[9],processed_fox_data[4], \"FOX News Video Interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to do a broad sentiment analysis for each channel's videos, we can write some functions to help us do that. We'll loop through our processed data and retrive the comments. For each video, we'll find the mean average positive, negative, and compound sentiment scores by aggregating these scores from each comment in the video. We'll also create an overloaded method for analyzing sentiments towards a specific keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def analyze(comment):\n",
    "    score = analyzer.polarity_scores(comment)\n",
    "    return score[\"compound\"], score[\"pos\"], score[\"neg\"]\n",
    "def analyze_channel(data):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    compound_scores = []\n",
    "    like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates,\\\n",
    "    titles, comments, view_nums, description, interactions = data\n",
    "    for author_dict in comments: # video comments\n",
    "        vid_pos = []\n",
    "        vid_neg = []\n",
    "        vid_comp = []\n",
    "        for author in author_dict: # all the authors in the comment section\n",
    "            for comment in author_dict[author]: # their comments\n",
    "                comp, pos, neg = analyze(comment[0])\n",
    "                vid_pos.append(pos)\n",
    "                vid_neg.append(neg)\n",
    "                vid_comp.append(comp)\n",
    "        pos = statistics.mean(vid_pos)\n",
    "        neg = statistics.mean(vid_neg)\n",
    "        comp = statistics.mean(vid_comp)\n",
    "        pos_scores.append(pos)\n",
    "        neg_scores.append(neg)\n",
    "        compound_scores.append(comp)\n",
    "    return pos_scores,neg_scores,compound_scores\n",
    "def analyze_channel_by_keyword(data, keyword):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    compound_scores = []\n",
    "    dates_by_keyword = []\n",
    "    like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates,\\\n",
    "    titles, comments, view_nums, descriptions, interactions = data\n",
    "    index = 0\n",
    "    for author_dict in comments: # video comments\n",
    "        if keyword in titles[index] or keyword in descriptions[index]:\n",
    "            vid_pos = []\n",
    "            vid_neg = []\n",
    "            vid_comp = []\n",
    "            for author in author_dict: # all the authors in the comment section\n",
    "                for comment in author_dict[author]: # their comments\n",
    "                    comp, pos, neg = analyze(comment[0])\n",
    "                    vid_pos.append(pos)\n",
    "                    vid_neg.append(neg)\n",
    "                    vid_comp.append(comp)\n",
    "            pos = statistics.mean(vid_pos)\n",
    "            neg = statistics.mean(vid_neg)\n",
    "            comp = statistics.mean(vid_comp)\n",
    "            pos_scores.append(pos)\n",
    "            neg_scores.append(neg)\n",
    "            compound_scores.append(comp)\n",
    "            dates_by_keyword.append(dates[index])\n",
    "        index+=1\n",
    "    return pos_scores,neg_scores,compound_scores,dates_by_keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll retrieve the sentiment data for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pos, cnn_neg, cnn_comp, cnn_dates = analyze_channel_by_keyword(processed_cnn_data, \"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_pos, fox_neg, fox_comp, fox_dates = analyze_channel_by_keyword(processed_fox_data, \"Trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll take the sentiment data we got from our channel analysis with a keyword parameter of \"Trump\" to see how CNN viewers and Fox viewers tend to speak when the President is mentioned in or is the subject of one of their Youtube videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_comp,cnn_dates, \"CNN Viewers Sentiments (Trump, compound score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(fox_comp,fox_dates, \"Fox Viewers Sentiments (Trump, compound score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_pos,cnn_dates,\"CNN Viewers Sentiments (Trump, positive score)\")\n",
    "plot_data(fox_pos,fox_dates,\"Fox Viewers Sentiments (Trump, positive score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_neg,cnn_dates,\"CNN Viewers Sentiments (Trump, negative score)\")\n",
    "plot_data(fox_neg,fox_dates,\"Fox Viewers Sentiments (Trump, negative score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
