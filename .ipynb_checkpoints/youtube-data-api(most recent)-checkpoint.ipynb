{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Collection\n",
    "\n",
    "First, we lay out some methods in order to treat our Youtube Data API keys as though they are being fed through a revolving door. If we run queries with an API object built from just one key, we hit quota limits that block us from downloading more information and the program crashes, destroying our data. With our system, an API object is built from the first key in the pool of API keys. When the program encounters an HTTP Error from Google, a new API object is built from the next key in the pool. The new object is passed back to the method running queries and the method carries on making requests to the API. The API keys are a bit like ammunition because once they've exhausted their allocated bandwith from Google, they are useless for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient import errors\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "# Author: Jared Prior, Cedric Blaise\n",
    "\n",
    "# API Revolving Door \n",
    "# intercepts rejected API requests and spins \n",
    "# up a new Youtube build using the next API\n",
    "# key in the pool. The rejected key is discarded\n",
    "# and not considered again, as the revolving\n",
    "# door only dispenses each API key from the \n",
    "# pool once.\n",
    "current_key = 0\n",
    "api_revolving_door = [\"AIzaSyCCCQsP3NGY9rGvOBI5unIvEy4OsAC41tc\", #1\n",
    "                      \"AIzaSyCPqYMsq1HOeOSaLtBGejUh5-GzOKDV8lg\", #2\n",
    "                      \"AIzaSyCGEBtZPu5Bpfa-wJIMcE6QPnzIYiNwD5k\", #3\n",
    "                      'AIzaSyB2VjIO1qBKtHIYfM1kLGK0X4huo5cPgJg', #4\n",
    "                      'AIzaSyDBAlVzk0Q_pqFqdxdosJ09AjS9RhN1o28', #5\n",
    "                      'AIzaSyAQBq0X5Q3JCwwAlqx7hP24x0tS6NYpZpE', #6\n",
    "                      'AIzaSyAQBq0X5Q3JCwwAlqx7hP24x0tS6NYpZpE', #7\n",
    "                      'AIzaSyDbQTp-qKJP55kAAEgXP2vD80uHK4fVG-s', #8\n",
    "                      'AIzaSyBmFEUq7B52ei7WYnlRbY2biTcNkEEIwsM', #9\n",
    "                      \"AIzaSyDbvMLfe7FPhCCycAcKQxWB2urU3SOD0qM\", #10\n",
    "                      \"AIzaSyBMlUoxwkfP2PIjIgDRCe7ladBN6efLgzw\", #11\n",
    "                      'AIzaSyA1Y_LCB5KCU6kZBJVZlHXSvmbNjGiitfY', #12\n",
    "                      'AIzaSyBlqtXeEgvyJGaw5h0SRxWO_ibw_JWX42s', #13\n",
    "                      \"AIzaSyCmXJ4LrPH9IPyw_CyPbmZF2947j6rBcIY\", #14\n",
    "                      \"AIzaSyBU3Hg6Ph7a-z-Dgh1eJ9tHjgDfAonxlyw\", #15\n",
    "                      \"AIzaSyDmKGvWEZzfJKo9QS0CAovzYCLZE3MpF7w\", #16\n",
    "                      \"AIzaSyAKbiwdli7mrYm-5Hcnl356PB8rGam8fB0\", #17\n",
    "                      \"AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk\", #18\n",
    "                      \"AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk\", #19\n",
    "                      \"AIzaSyAXGe4C55z3hL8Znu2vKkQzjUaV0HgWfhs\", #20\n",
    "                      \"AIzaSyCPj4BPl3XcqR8L1GMEblZKKJds3hVIKm8\", #21\n",
    "                      \"AIzaSyACcJRNByz_GLpvZGrdl5RjCtJiH2UGDbo\", #22\n",
    "                      \"AIzaSyDEJYq2pJr6C-czt_jv4g5lf3ni3gharLA\", #23\n",
    "                      \"AIzaSyCMDeQQJogAafO9r3t3yp0Mnp5Spk4K1WU\",\n",
    "                      \"AIzaSyCFo4tjFT1Mv5I6tF6Gm3JbDjlv4rjXefY\", \n",
    "                      \"AIzaSyBn0Ad5kaZx2eyArelLLMKNjDSg4ogZ2SA\", \n",
    "                      \"AIzaSyADUMqQ4Vc4zk208Tax8t0adnFi9nGyi90\"]\n",
    "def yt_build(key):\n",
    "    # builds Youtube object from API key\n",
    "    yt = build(\"youtube\",'v3',developerKey=key) # establishes a connection with the Youtube Data API v3\n",
    "    return yt\n",
    "def door_spin():\n",
    "    global current_key\n",
    "    # retrieves the next build or flags empty pool\n",
    "    if current_key > (len(api_revolving_door)-2):\n",
    "        return None\n",
    "    else:\n",
    "        current_key += 1\n",
    "    print(\"Spinning up new API build: \" + str(current_key))\n",
    "    api_key = api_revolving_door[current_key]\n",
    "    yt = yt_build(api_key)\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = yt_build(api_revolving_door[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we craft a function to retrieve comments from any given \n",
    "video by its video ID. This function returns all of the comments in a \n",
    "dictionary, where the comment authors are keys. Each of their comments\n",
    "and their number of likes are stored as the values in a heterogenous list. The video statistics are also retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_comments(video_id, yt):\n",
    "    # retrieves comments from a given Youtube video by its ID\n",
    "    # returns a dictionary of authors and their comments\n",
    "    global current_key\n",
    "    authors = {}\n",
    "    def exists(author_id):\n",
    "        return author_id in authors\n",
    "    stats = yt.videos().list(id=video_id,part=\"statistics\").execute()['items'][0]['statistics']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    pages = 5\n",
    "    while True:\n",
    "        res = yt.commentThreads().list(videoId=video_id,\n",
    "                                       part=\"snippet,replies\",maxResults=100,pageToken = next_page).execute()\n",
    "\n",
    "        for comment_data in res['items']:\n",
    "            comment_data = comment_data['snippet']['topLevelComment']['snippet']\n",
    "            comment = comment_data['textOriginal']\n",
    "            likes = comment_data['likeCount']\n",
    "            author = comment_data['authorDisplayName']\n",
    "            if exists(author):\n",
    "                authors[author].append([comment,likes])\n",
    "            else:\n",
    "                authors[author] = []\n",
    "                authors[author].append([comment,likes])\n",
    "        next_page = res.get('nextPageToken')\n",
    "        page +=1\n",
    "        if next_page==None or page > pages:\n",
    "            break\n",
    "    return authors, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function retrieves all of a given channel's Youtube\n",
    "videos and stores each video's video ID, title, date of publishing,\n",
    "its description, and its statistics. It uses the get_video_comments method to \n",
    "create a dictionary of all the comments, and it stores all of these\n",
    "attributes as a heterogenous list within a list of other video-lists. I personally think this method is ugly and I will look into thinning it out and making it more legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_reason(err):\n",
    "    if err.resp.get('content-type', '').startswith('application/json'):\n",
    "        reason = json.loads(err.content).get('error').get('errors')[0].get('reason')\n",
    "    else:\n",
    "        reason=\"Unexplained\"\n",
    "    return reason\n",
    "def channel_videos(channel_id, yt, pages):\n",
    "    # returns all the videos of a particular Youtube\n",
    "    # channel, and returns data for each video (title, description,\n",
    "    # comments, date, statistics, etc.)\n",
    "    global current_key\n",
    "    print(\"Retrieving data for \" + channel_id)\n",
    "    res = None\n",
    "    while res == None:\n",
    "        try:\n",
    "            res = yt.channels().list(id=channel_id,part='contentDetails').execute()\n",
    "        except errors.HttpError as error:\n",
    "            if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                print(\"Disabled comments: %s\" % error)\n",
    "                pass\n",
    "            elif error.resp.status == 403:\n",
    "                print(\"Error1: %s\" % error)\n",
    "                yt = door_spin()\n",
    "                if yt == None:\n",
    "                    print(\"Breaking\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error2: %s\" % error)\n",
    "                pass\n",
    "    items = res['items']\n",
    "    playlist_id = items[0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    videos = []\n",
    "    while True:\n",
    "        print(\"Current API index: \" + str(current_key))\n",
    "        try:\n",
    "            result = yt.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,\n",
    "                                             pageToken = next_page).execute()\n",
    "            for res in result['items']:\n",
    "                video_id = res['snippet']['resourceId']['videoId']\n",
    "                video_title = res['snippet']['title']\n",
    "                video_publish_date = res['snippet']['publishedAt']\n",
    "                video_description = res['snippet']['description']\n",
    "                if random.random() > 0.20:\n",
    "                    try:\n",
    "                        authors, stats = get_video_comments(video_id, yt)\n",
    "                        videos.append([video_title, video_publish_date, video_description, stats, authors])\n",
    "                    except errors.HttpError as error:\n",
    "                        if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                            print(\"Disabled comments: %s\" % error)\n",
    "                            pass\n",
    "                        elif error.resp.status == 403:\n",
    "                            print(\"Error1 inner: %s\" % error)\n",
    "                            yt = door_spin()\n",
    "                            if yt == None:\n",
    "                                print(\"Breaking\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(\"Error2: %s\" % error)\n",
    "                            pass\n",
    "            if random.random()>0.80:\n",
    "                next_page = yt.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,\n",
    "                pageToken = result.get('nextPageToken')).execute().get('nextPageToken')\n",
    "            else:\n",
    "                next_page = result.get('nextPageToken')\n",
    "                page += 1\n",
    "                print(\"Current page: \" + str(page))\n",
    "        except errors.HttpError as error:\n",
    "            if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                print(\"Disabled comments: %s\" % error)\n",
    "                pass\n",
    "            elif error.resp.status == 403:\n",
    "                print(\"Error1 outer: %s\" % error)\n",
    "                yt = door_spin()\n",
    "                if yt == None:\n",
    "                    print(\"Breaking\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error2: %s\" % error)\n",
    "                continue\n",
    "        if next_page == None or page > pages:\n",
    "            break\n",
    "        print(str(len(videos)) + \" videos retrieved from the API so far\")\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some channels might appear as a \"user\" instead of a \"channel,\" so we convert those by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_id = yt.channels().list(part=\"id\",forUsername='CNN').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_id = yt.channels().list(part=\"id\",forUsername='FoxNewsChannel').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_times_id = yt.channels().list(part=\"id\",forUsername='TheNewYorkTimes').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_id = yt.channels().list(part=\"id\",forUsername='msnbcleanforward').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart_id = \"UCmgnsaQIK1IR808Ebde-ssA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we retrieve the videos using our channel IDs and our channel_videos method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = channel_videos(cnn_id, yt, 20) # left wing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fox_data = channel_videos(fox_id, yt, 20) # right wing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_times_data = channel_videos(ny_times_id, yt, 20) # left wing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_data = channel_videos(msnbc_id, yt, 20) # left wing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart_data = channel_videos(breitbart_id, yt, 20) # alt right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Caching\n",
    "We will now process our data and organize it so that \n",
    "it may be used for analysis. We will also cache our data\n",
    "at this point, since Google restricts the number of requests\n",
    "we can make with the API and we want to make the most of\n",
    "every request we are allowed. It also takes a frustratingly long\n",
    "time to make the requests and store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_channel_data(data, channel):\n",
    "    channel_file = open(channel +\".txt\",\"w\")\n",
    "    channel_file.write(str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will include an unpackaging \n",
    "method so that we can retrieve the data from our text files\n",
    "in the same format as it is returned in the channel_videos\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_cached_data(channel):\n",
    "    print(\"Unpacking data: \", channel)\n",
    "    channel_file = open(channel, \"r\")\n",
    "    cached_data = channel_file.read()\n",
    "    videos = ast.literal_eval(cached_data)\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use our caching function to dump our data to text files. We can use the unpacking method to retrieve all the data in its pre-cache format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(cnn_data, \"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(fox_data, \"Fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(breitbart_data, \"Breitbart News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(ny_times_data, \"NYTimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(msnbc_data, \"MSNBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucnn_data = unpack_cached_data(\"CNN.txt\")\n",
    "ufox_data = unpack_cached_data(\"Fox.txt\")\n",
    "ubreitbart_data = unpack_cached_data(\"Breitbart News.txt\")\n",
    "umsnbc_data = unpack_cached_data(\"MSNBC.txt\")\n",
    "uny_times_data = unpack_cached_data(\"NYTimes.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Processing and Organization\n",
    "We'll write a function to process the statistics for each video and return the number of views, likes, dislikes, and comments. But first we will need some helper methods to help us convert the video dates and timestamps to the proper format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def convert_raw_date(date):\n",
    "    date_raw = date.split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_raw[0])\n",
    "    month = int(date_raw[1])\n",
    "    day = int(date_raw[2])\n",
    "    return year, month, day\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_stats(video):\n",
    "    title, publish_date, description, stats, authors = video\n",
    "    year, month, day = convert_raw_date(publish_date)\n",
    "    date = datetime.date(year,month,day)\n",
    "    views = stats['viewCount']\n",
    "    likes = stats['likeCount']\n",
    "    dislikes = stats['dislikeCount']\n",
    "    num_comments = stats['commentCount']\n",
    "    return title, date, description, int(views), int(likes), int(dislikes), int(num_comments), authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our processing function to build up lists of different values that we can plot against the publishing dates of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(videos):\n",
    "    # returns a list of lists where index i in any of the lists\n",
    "    # contains specific information about video i e.g,\n",
    "    # titles[i], like_nums[i] = gives you the title and \n",
    "    # number of likes of video i\n",
    "    like_nums = []\n",
    "    dislike_nums = []\n",
    "    comment_nums = []\n",
    "    dislike_ratios = []\n",
    "    dates = []\n",
    "    titles = []\n",
    "    comments = []\n",
    "    view_nums = []\n",
    "    interactions = []\n",
    "    descriptions = []\n",
    "    for video in videos:\n",
    "        title, date, description, views, likes, dislikes, num_comments, authors=process_video_stats(video)\n",
    "        like_nums.append(likes)\n",
    "        dislike_nums.append(dislikes)\n",
    "        dislike_ratios.append(dislikes/(likes+dislikes))\n",
    "        comment_nums.append(num_comments)\n",
    "        dates.append(date)\n",
    "        titles.append(title)\n",
    "        comments.append(authors)\n",
    "        view_nums.append(views)\n",
    "        interactions.append(num_comments+likes+dislikes)\n",
    "        descriptions.append(description)\n",
    "    return like_nums, dislike_nums, comment_nums, dislike_ratios,\\\n",
    "    dates, titles, comments, view_nums, descriptions, interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can process all of our data so that it is more accessible and easier to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cnn_data = process_videos(cnn_data)\n",
    "processed_fox_data = process_videos(fox_data)\n",
    "processed_nytimes_data = process_videos(ny_times_data)\n",
    "processed_msnbc_data = process_videos(msnbc_data)\n",
    "processed_breitbart_data = process_videos(breitbart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write some auxiliary functions to help us merge authors amongst all the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_channel_authors(author_dicts):\n",
    "    author_dict = {}\n",
    "    for authors in author_dicts:\n",
    "        for author in authors:\n",
    "            if author in author_dict:\n",
    "                author_dict[author].append(authors[author])\n",
    "    return author_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_authors(comment_dictionaries):\n",
    "    unique_authors = {}\n",
    "    for video in comments:\n",
    "        for author in video:\n",
    "            if author in unique_authors:\n",
    "                unique_authors[author].append(video[author])\n",
    "            else:\n",
    "                unique_authors[author] = []\n",
    "                unique_authors[author].append(video[author])\n",
    "    return unique_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(y, title):\n",
    "    file=open(title,\"w\")\n",
    "    for x in y:\n",
    "        file.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Analysis and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now write a plotting function to plot based on the metrics we've produced from our processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "%matplotlib inline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import datetime\n",
    "from matplotlib.dates import WeekdayLocator\n",
    "from matplotlib.dates import DayLocator\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,rrulewrapper, RRuleLocator, drange)\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "import statistics\n",
    "def plot_data(data, dates, title):\n",
    "    mpl.rcParams['figure.dpi'] = 150\n",
    "    loc = DayLocator(interval=30)\n",
    "    formatter = DateFormatter('%Y-%m-%d')\n",
    "    fig, ax = plt.subplots()\n",
    "    y_mean = statistics.mean(data)\n",
    "    st=\"{:.2f}\".format(y_mean)\n",
    "    plt.plot_date(np.array(dates), np.array(data))\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_tick_params(rotation=30, labelsize=10)\n",
    "    plt.axhline(y=y_mean, color='r', linestyle='-',label='Mean Average: {}'.format(float(st)))\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,5))\n",
    "    style.use('fivethirtyeight')\n",
    "    plt.show()\n",
    "def plot_datas(data, data2, title):\n",
    "    mpl.rcParams['figure.dpi'] = 150\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "    style.use('fivethirtyeight')\n",
    "    plt.scatter(data,data2,s=2)\n",
    "    z = numpy.polyfit(data, data2, 1)\n",
    "    p = numpy.poly1d(z)\n",
    "    #plt.plot(data,p(data),\"r--\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to plot video statistics over time from any given channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[0],processed_cnn_data[4], \"CNN Video Likes\")\n",
    "plot_data(processed_fox_data[0],processed_fox_data[4], \"FOX News Video Likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[1],processed_cnn_data[4], \"CNN Video Dislikes\")\n",
    "plot_data(processed_fox_data[1],processed_fox_data[4], \"FOX News Video Dislikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[3],processed_cnn_data[4], \"CNN Video Dislikes/Total\")\n",
    "plot_data(processed_fox_data[3],processed_fox_data[4], \"FOX News Video Dislikes/Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[7],processed_cnn_data[4], \"CNN Video Views\")\n",
    "plot_data(processed_fox_data[7],processed_fox_data[4], \"FOX News Video Views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[2],processed_cnn_data[4], \"CNN Video Comments\")\n",
    "plot_data(processed_fox_data[2],processed_fox_data[4], \"FOX News Video Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[9],processed_cnn_data[4], \"CNN Video Interactions\")\n",
    "plot_data(processed_fox_data[9],processed_fox_data[4], \"FOX News Video Interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to do a broad sentiment analysis for each channel's videos, we can write some functions to help us do that. We'll loop through our processed data and retrive the comments. For each video, we'll find the mean average positive, negative, and compound sentiment scores by aggregating these scores from each comment in the video. We'll also create an overloaded method for analyzing sentiments towards a specific keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def analyze(comment):\n",
    "    score = analyzer.polarity_scores(comment)\n",
    "    return score[\"compound\"], score[\"pos\"], score[\"neg\"]\n",
    "def analyze_channel(data):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    compound_scores = []\n",
    "    like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates,\\\n",
    "    titles, comments, view_nums, description, interactions = data\n",
    "    for author_dict in comments: # video comments\n",
    "        vid_pos = []\n",
    "        vid_neg = []\n",
    "        vid_comp = []\n",
    "        for author in author_dict: # all the authors in the comment section\n",
    "            for comment in author_dict[author]: # their comments\n",
    "                comp, pos, neg = analyze(comment[0])\n",
    "                vid_pos.append(pos)\n",
    "                vid_neg.append(neg)\n",
    "                vid_comp.append(comp)\n",
    "        pos = statistics.mean(vid_pos)\n",
    "        neg = statistics.mean(vid_neg)\n",
    "        comp = statistics.mean(vid_comp)\n",
    "        pos_scores.append(pos)\n",
    "        neg_scores.append(neg)\n",
    "        compound_scores.append(comp)\n",
    "    return pos_scores,neg_scores,compound_scores\n",
    "def analyze_channel_by_keyword(data, keyword):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    compound_scores = []\n",
    "    dates_by_keyword = []\n",
    "    like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates,\\\n",
    "    titles, comments, view_nums, descriptions, interactions = data\n",
    "    index = 0\n",
    "    for author_dict in comments: # video comments\n",
    "        if keyword in titles[index] or keyword in descriptions[index]:\n",
    "            vid_pos = []\n",
    "            vid_neg = []\n",
    "            vid_comp = []\n",
    "            for author in author_dict: # all the authors in the comment section\n",
    "                for comment in author_dict[author]: # their comments\n",
    "                    comp, pos, neg = analyze(comment[0])\n",
    "                    vid_pos.append(pos)\n",
    "                    vid_neg.append(neg)\n",
    "                    vid_comp.append(comp)\n",
    "            pos = statistics.mean(vid_pos)\n",
    "            neg = statistics.mean(vid_neg)\n",
    "            comp = statistics.mean(vid_comp)\n",
    "            pos_scores.append(pos)\n",
    "            neg_scores.append(neg)\n",
    "            compound_scores.append(comp)\n",
    "            dates_by_keyword.append(dates[index])\n",
    "        index+=1\n",
    "    return pos_scores,neg_scores,compound_scores,dates_by_keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll retrieve the sentiment data for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pos, cnn_neg, cnn_comp, cnn_dates = analyze_channel_by_keyword(processed_cnn_data, \"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_pos, fox_neg, fox_comp, fox_dates = analyze_channel_by_keyword(processed_fox_data, \"Trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll take the sentiment data we got from our channel analysis with a keyword parameter of \"Trump\" to see how CNN viewers and Fox viewers tend to speak when the President is mentioned in or is the subject of one of their Youtube videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_pos,cnn_dates, \"CNN Viewer Sentiments (Trump, positive score)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(fox_comp,fox_dates, \"Fox Viewer Sentiments (Trump, compound score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_pos,cnn_dates,\"CNN Viewer Sentiments (Trump, positive score)\")\n",
    "plot_data(fox_pos,fox_dates,\"Fox Viewer Sentiments (Trump, positive score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_neg,cnn_dates,\"CNN Viewer Sentiments (Trump, negative score)\")\n",
    "plot_data(fox_neg,fox_dates,\"Fox Viewer Sentiments (Trump, negative score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores,neg_scores,compound_scores = analyze_channel(processed_cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[3],pos_scores, \"Dislike Ratio Against Positive Sentiment Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[3],neg_scores, \"Dislike Ratio Against Negative Sentiment Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[3],compound_scores, \"Dislike Ratio Against Compound Sentiment Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[9],compound_scores, \"Interactions vs. Compound Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Data Pre-Processing for Classification\n",
    "We'll need to tokenize our data for processing. We'll begin with tokenizing our video titles and descriptions, as they'll need to be narrowed down to keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = [(processed_cnn_data, \"CNN\"),\n",
    "            (processed_fox_data, \"FOX\"),\n",
    "            (processed_msnbc_data, \"MSNBC\"),\n",
    "            (processed_breitbart_data, \"Breitbart\"),\n",
    "            (processed_nytimes_data, \"NYTimes\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of data, if we want to reduce running time, we can abbreviate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviated_data_set = []\n",
    "for data in data_set:\n",
    "    a,b,c,d,e,f,g,h,i,j = data[0]\n",
    "    channel = data[1]\n",
    "    data = a[::2],b[::2],c[::2],\\\n",
    "    d[::2],e[::2],f[::2],g[::2],\\\n",
    "    h[::2],i[::2],j[::2]\n",
    "    abbreviated_data_set.append((data,channel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = []\n",
    "for data in abbreviated_data_set:\n",
    "    label_names.append(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is relatively large (certainly, it's massive for my 8GBRAM Macbook Pro), so finalizing the pre-processing might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "def create_comment_frame(data,channel):\n",
    "    comment_list = []\n",
    "    i = 0\n",
    "    for comment_dict in data[6]: # for every video comment dictionary\n",
    "        likes = data[0][i]\n",
    "        dislikes = data[1][i]\n",
    "        comment_nums = data[2][i]\n",
    "        title = data[5][i]\n",
    "        views = data[7][i]\n",
    "        description = data[8][i]\n",
    "        for author in comment_dict: # for every author in each vid. dictionary\n",
    "            for comment in comment_dict[author]: # for all their comments\n",
    "                comment_list.append((comment[0],\n",
    "                                     comment[1],\n",
    "                                     author,\n",
    "                                     likes,\n",
    "                                     dislikes,\n",
    "                                     comment_nums,\n",
    "                                     title, \n",
    "                                     description,\n",
    "                                     views)) # append their comment and its data to the list\n",
    "        i+=1\n",
    "    pd_dict = {}\n",
    "    pd_dict['comment'] = []\n",
    "    pd_dict['commentLikes'] = []\n",
    "    pd_dict['authorName'] = []\n",
    "    pd_dict['title'] = []\n",
    "    pd_dict['description'] = []\n",
    "    pd_dict['videoLikes'] = []\n",
    "    pd_dict['videoDislikes'] = []\n",
    "    pd_dict['views'] = []\n",
    "    pd_dict['commentNum'] = []\n",
    "    for comment in comment_list:\n",
    "        pd_dict['comment'].append(comment[0])\n",
    "        pd_dict['commentLikes'].append(comment[1])\n",
    "        pd_dict['authorName'].append(comment[2])\n",
    "        pd_dict['videoLikes'].append(comment[3])\n",
    "        pd_dict['videoDislikes'].append(comment[4])\n",
    "        pd_dict['commentNum'].append(comment[5])\n",
    "        pd_dict['title'].append(comment[6])\n",
    "        pd_dict['description'].append(comment[7])\n",
    "        pd_dict['views'].append(comment[8])\n",
    "    channel_comments = pd.DataFrame.from_dict(pd_dict)\n",
    "    channel_comments['channel'] = channel\n",
    "    return channel_comments\n",
    "def create_comment_dataframe(dataset):\n",
    "    frame = create_comment_frame(dataset[0][0], dataset[0][1])\n",
    "    for data in dataset[1:]:\n",
    "        frame = pd.concat([frame,create_comment_frame(data[0], data[1])]).sample(frac=1).reset_index(drop=True)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_dataframe = create_comment_dataframe(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>commentLikes</th>\n",
       "      <th>authorName</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>videoLikes</th>\n",
       "      <th>videoDislikes</th>\n",
       "      <th>views</th>\n",
       "      <th>commentNum</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NPV is an initiative that is underway to do aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>Self Learning</td>\n",
       "      <td>McCarthy on impeachment: 'yesterday was no cau...</td>\n",
       "      <td>House Minority Leader McCarthy holds his weekl...</td>\n",
       "      <td>6698</td>\n",
       "      <td>204</td>\n",
       "      <td>308208</td>\n",
       "      <td>1318</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you know your training your replacement tea...</td>\n",
       "      <td>0</td>\n",
       "      <td>Grant Smith</td>\n",
       "      <td>'Tucker Carlson Tonight' investigates: Are US ...</td>\n",
       "      <td>AT&amp;T outsourcing American jobs, forcing employ...</td>\n",
       "      <td>7652</td>\n",
       "      <td>216</td>\n",
       "      <td>240931</td>\n",
       "      <td>2618</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Macau, HK, Tibet... Japan,Portugal,UK,USA atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ronald Dumb</td>\n",
       "      <td>New York Times reporter: Uyghurs say Xinjiang ...</td>\n",
       "      <td>CNN's Rosemary Church interviews Austin Ramzy,...</td>\n",
       "      <td>879</td>\n",
       "      <td>264</td>\n",
       "      <td>78196</td>\n",
       "      <td>1318</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has anyone asked Cuomo if he wants to be the n...</td>\n",
       "      <td>0</td>\n",
       "      <td>John Whale</td>\n",
       "      <td>Brit Hume analyzes the impact of coronavirus c...</td>\n",
       "      <td>Fox News senior political analyst Brit Hume jo...</td>\n",
       "      <td>1501</td>\n",
       "      <td>142</td>\n",
       "      <td>91328</td>\n",
       "      <td>924</td>\n",
       "      <td>FOX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nobody cares about basketball anymore. Whateve...</td>\n",
       "      <td>0</td>\n",
       "      <td>mark tito</td>\n",
       "      <td>Stephen A. Smith makes prediction about NBA se...</td>\n",
       "      <td>ESPN's Stephen A. Smith explains why he believ...</td>\n",
       "      <td>725</td>\n",
       "      <td>193</td>\n",
       "      <td>84396</td>\n",
       "      <td>663</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  commentLikes  \\\n",
       "0  NPV is an initiative that is underway to do aw...             0   \n",
       "1  If you know your training your replacement tea...             0   \n",
       "2  Macau, HK, Tibet... Japan,Portugal,UK,USA atta...             0   \n",
       "3  Has anyone asked Cuomo if he wants to be the n...             0   \n",
       "4  Nobody cares about basketball anymore. Whateve...             0   \n",
       "\n",
       "      authorName                                              title  \\\n",
       "0  Self Learning  McCarthy on impeachment: 'yesterday was no cau...   \n",
       "1    Grant Smith  'Tucker Carlson Tonight' investigates: Are US ...   \n",
       "2    Ronald Dumb  New York Times reporter: Uyghurs say Xinjiang ...   \n",
       "3     John Whale  Brit Hume analyzes the impact of coronavirus c...   \n",
       "4      mark tito  Stephen A. Smith makes prediction about NBA se...   \n",
       "\n",
       "                                         description  videoLikes  \\\n",
       "0  House Minority Leader McCarthy holds his weekl...        6698   \n",
       "1  AT&T outsourcing American jobs, forcing employ...        7652   \n",
       "2  CNN's Rosemary Church interviews Austin Ramzy,...         879   \n",
       "3  Fox News senior political analyst Brit Hume jo...        1501   \n",
       "4  ESPN's Stephen A. Smith explains why he believ...         725   \n",
       "\n",
       "   videoDislikes   views  commentNum channel  \n",
       "0            204  308208        1318     FOX  \n",
       "1            216  240931        2618     FOX  \n",
       "2            264   78196        1318     CNN  \n",
       "3            142   91328         924     FOX  \n",
       "4            193   84396         663     CNN  "
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Classification\n",
    "So we have our dataframes all loaded up! Pandas is great because it allows us to perform vectorized operations on our data, which is fairly dense. We will divide our dataframe into a training and test set and we will create text embeddings for our features. I've adapted some code used for sentiment analysis to suit our purposes. I've organized things so that our model considers each comment's content and its context (so the actual language and the title, description, and statistics of the video it was left on. The model treats the channel names as labels, so when it is trained and shown comments, it can make predictions about which channel the comments originated from. Here, we will first train the model to try to recognize channel origins for just singular comments. Then, we will merge comments to form a corpus of comment-books for each video, treating a video as though it were an author of a book of all of its comments, and we will try to train the model to recognize which channel a body of comments came from.\n",
    "\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "\n",
    "We will be using a Deep Neural Network classifier from TensorFlow. The folks at Google's TensorFlow Hub put together a Jupyter notebook in which they trained a classifier to classify sentiment polarity in movie reviews. I adapted this model so that it can use multiple labels (our channel names), and multiple features (the fields from our dataframes, which I created from our processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(all_comments_dataframe, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll divide our dataframe into a test set and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the whole training set.\n",
    "train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "    train_df, train_df[\"channel\"], num_epochs=None, shuffle=True)\n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "    train_df, train_df[\"channel\"], shuffle=False)\n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
    "    test_df, test_df[\"channel\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create embeddings for all our features: comments, video descriptions, the number of likes for each comment, the author of the comment, and the title of the video the comment was left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_feature_column = hub.text_embedding_column(\n",
    "    key=\"comment\", \n",
    "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "embedded_text_feature_column2 = hub.text_embedding_column(\n",
    "    key=\"description\", \n",
    "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "embedded_text_feature_column3 = tf.feature_column.numeric_column(\n",
    "    \"commentLikes\", shape=(1,), default_value=None, dtype=tf.dtypes.float32, normalizer_fn=None)\n",
    "embedded_text_feature_column4 = hub.text_embedding_column(\n",
    "    key=\"authorName\", \n",
    "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")\n",
    "embedded_text_feature_column5 = hub.text_embedding_column(\n",
    "    key=\"title\", \n",
    "    module_spec=\"https://tfhub.dev/google/nnlm-en-dim128/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will pass our feautures and labels to a TensorFlow DNN estimator and fire up the estimator so that we can run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[500, 100],\n",
    "    feature_columns=[embedded_text_feature_column,\n",
    "                     embedded_text_feature_column2,\n",
    "                     embedded_text_feature_column3,\n",
    "                     embedded_text_feature_column4,\n",
    "                     embedded_text_feature_column5],\n",
    "    label_vocabulary = label_names,\n",
    "    n_classes=5,\n",
    "    optimizer=tf.keras.optimizers.Adagrad(lr=0.003))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-05-05T20:46:02Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-05-05T20:46:02Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 114.23019s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 114.23019s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-05-05-20:47:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-05-05-20:47:56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9855486, average_loss = 0.06835699, global_step = 5000, loss = 0.06835705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9855486, average_loss = 0.06835699, global_step = 5000, loss = 0.06835705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-05-05T20:47:57Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2020-05-05T20:47:57Z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 34.36947s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Inference Time : 34.36947s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-05-05-20:48:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2020-05-05-20:48:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9858319, average_loss = 0.06781122, global_step = 5000, loss = 0.0678078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9858319, average_loss = 0.06781122, global_step = 5000, loss = 0.0678078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /var/folders/tn/fcl1261n75l3455rc43h8df80000gn/T/tmps6lcsp55/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9855486154556274\n",
      "Test set accuracy: 0.9858319163322449\n"
     ]
    }
   ],
   "source": [
    "train_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\n",
    "test_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\n",
    "print(\"Training set accuracy: {accuracy}\".format(**train_eval_result))\n",
    "print(\"Test set accuracy: {accuracy}\".format(**test_eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem too good to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = {}\n",
    "test_dataframe['comment'] = []\n",
    "test_dataframe['commentLikes'] = []\n",
    "test_dataframe['authorName'] = []\n",
    "test_dataframe['title'] = []\n",
    "test_dataframe['description'] = []\n",
    "test_dataframe['videoLikes'] = []\n",
    "test_dataframe['videoDislikes'] = []\n",
    "test_dataframe['views'] = []\n",
    "test_dataframe['commentNum'] = []\n",
    "comment_list = [[\"This is the DUMBEST president ever in HISTORY! 🤔\",\n",
    "                 80,\n",
    "                 \"MagicSantos\",\n",
    "                 452,\n",
    "                 53,\n",
    "                 383,\n",
    "                 \"Trump explains why he plans to wind down the coronavirus task force\",\n",
    "                 \"CNN's Anderson Cooper reports that President Donald Trump plans to wind down the coronavirus task force near the end of May. #CNN #News\",\n",
    "                 4970]]\n",
    "for comment in comment_list:\n",
    "    test_dataframe['comment'].append(comment[0])\n",
    "    test_dataframe['commentLikes'].append(comment[1])\n",
    "    test_dataframe['authorName'].append(comment[2])\n",
    "    test_dataframe['videoLikes'].append(comment[3])\n",
    "    test_dataframe['videoDislikes'].append(comment[4])\n",
    "    test_dataframe['commentNum'].append(comment[5])\n",
    "    test_dataframe['title'].append(comment[6])\n",
    "    test_dataframe['description'].append(comment[7])\n",
    "    test_dataframe['views'].append(comment[8])\n",
    "test_dataframe = pd.DataFrame.from_dict(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
