{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Collection\n",
    "\n",
    "First, we lay out some methods in order to treat our API keys as though they are being fed through a revolving door. If we run queries with an API object built from just one key, we hit quota limits that block us from downloading more information and the program crashes, destroying our data. With our system, an API object is built from the first key in the pool of API keys. When the program encounters an HTTP Error from Google, a new API object is built from the next key in the pool. The new object is passed back to the method running queries and the method carries on making requests to the API. The API keys are a bit like ammunition because once they've exhausted their allocated bandwith from Google, they are useless for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "from apiclient import errors\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "# Author: Jared Prior, Cedric Blaise\n",
    "\n",
    "# API Revolving Door \n",
    "# intercepts rejected API requests and spins \n",
    "# up a new Youtube build using the next API\n",
    "# key in the pool. The rejected key is discarded\n",
    "# and not considered again, as the revolving\n",
    "# door only dispenses each API key from the \n",
    "# pool once.\n",
    "current_key = 0\n",
    "api_revolving_door = [\"AIzaSyCCCQsP3NGY9rGvOBI5unIvEy4OsAC41tc\",\n",
    "                      \"AIzaSyCPqYMsq1HOeOSaLtBGejUh5-GzOKDV8lg\",\n",
    "                      \"AIzaSyCGEBtZPu5Bpfa-wJIMcE6QPnzIYiNwD5k\",\n",
    "                      'AIzaSyB2VjIO1qBKtHIYfM1kLGK0X4huo5cPgJg',\n",
    "                      'AIzaSyDBAlVzk0Q_pqFqdxdosJ09AjS9RhN1o28',\n",
    "                      'AIzaSyAQBq0X5Q3JCwwAlqx7hP24x0tS6NYpZpE',\n",
    "                      'AIzaSyAQBq0X5Q3JCwwAlqx7hP24x0tS6NYpZpE',\n",
    "                      'AIzaSyDbQTp-qKJP55kAAEgXP2vD80uHK4fVG-s',\n",
    "                      'AIzaSyBmFEUq7B52ei7WYnlRbY2biTcNkEEIwsM',\n",
    "                      \"AIzaSyDbvMLfe7FPhCCycAcKQxWB2urU3SOD0qM\", \n",
    "                      \"AIzaSyBMlUoxwkfP2PIjIgDRCe7ladBN6efLgzw\", \n",
    "                      'AIzaSyA1Y_LCB5KCU6kZBJVZlHXSvmbNjGiitfY',\n",
    "                      'AIzaSyBlqtXeEgvyJGaw5h0SRxWO_ibw_JWX42s',\n",
    "                      \"AIzaSyCmXJ4LrPH9IPyw_CyPbmZF2947j6rBcIY\",\n",
    "                      \"AIzaSyBU3Hg6Ph7a-z-Dgh1eJ9tHjgDfAonxlyw\",\n",
    "                      \"AIzaSyDmKGvWEZzfJKo9QS0CAovzYCLZE3MpF7w\",\n",
    "                      \"AIzaSyAKbiwdli7mrYm-5Hcnl356PB8rGam8fB0\", \n",
    "                      \"AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk\",\n",
    "                      \"AIzaSyCfm7NmDGTRfuxMK6hMABJYoKdTRGZh-Jk\",\n",
    "                      \"AIzaSyAXGe4C55z3hL8Znu2vKkQzjUaV0HgWfhs\", \n",
    "                      \"AIzaSyCPj4BPl3XcqR8L1GMEblZKKJds3hVIKm8\", \n",
    "                      \"AIzaSyACcJRNByz_GLpvZGrdl5RjCtJiH2UGDbo\"]\n",
    "def yt_build(key):\n",
    "    # builds Youtube object from API key\n",
    "    yt = build(\"youtube\",'v3',developerKey=key) # establishes a connection with the Youtube Data API v3\n",
    "    return yt\n",
    "def door_spin():\n",
    "    global current_key\n",
    "    # retrieves the next build or flags empty pool\n",
    "    if current_key > (len(api_revolving_door)-2):\n",
    "        return None\n",
    "    else:\n",
    "        current_key += 1\n",
    "    print(\"Spinning up new API build: \" + str(current_key))\n",
    "    api_key = api_revolving_door[current_key]\n",
    "    yt = yt_build(api_key)\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = yt_build(api_revolving_door[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we craft a function to retrieve comments from any given \n",
    "video by its video ID. This function returns all of the comments in a \n",
    "dictionary, where the comment authors are keys. Each of their comments\n",
    "and their number of likes are stored as the values in a heterogenous list. The video statistics are also retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_reason(err):\n",
    "    if err.resp.get('content-type', '').startswith('application/json'):\n",
    "        reason = json.loads(err.content).get('error').get('errors')[0].get('reason')\n",
    "    else:\n",
    "        reason=\"Unexplained\"\n",
    "    return reason\n",
    "def get_video_comments(video_id, yt):\n",
    "    # retrieves comments from a given Youtube video by its ID\n",
    "    # returns a dictionary of authors and their comments\n",
    "    global current_key\n",
    "    authors = {}\n",
    "    def exists(author_id):\n",
    "        return author_id in authors\n",
    "    stats = yt.videos().list(id=video_id,part=\"statistics\").execute()['items'][0]['statistics']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    pages = 5\n",
    "    while True:\n",
    "        res = yt.commentThreads().list(videoId=video_id,\n",
    "                                       part=\"snippet,replies\",maxResults=100,pageToken = next_page).execute()\n",
    "\n",
    "        for comment_data in res['items']:\n",
    "            comment_data = comment_data['snippet']['topLevelComment']['snippet']\n",
    "            comment = comment_data['textOriginal']\n",
    "            likes = comment_data['likeCount']\n",
    "            author = comment_data['authorDisplayName']\n",
    "            if exists(author):\n",
    "                authors[author].append([comment,likes])\n",
    "            else:\n",
    "                authors[author] = []\n",
    "                authors[author].append([comment,likes])\n",
    "        next_page = res.get('nextPageToken')\n",
    "        page +=1\n",
    "        if next_page==None or page > pages:\n",
    "            break\n",
    "    return authors, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function retrieves all of a given channel's Youtube\n",
    "videos and stores each video's video ID, title, date of publishing,\n",
    "its description, and its statistics. It uses the get_video_comments method to \n",
    "create a dictionary of all the comments, and it stores all of these\n",
    "attributes as a heterogenous list within a list of other video-lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_videos(channel_id, yt):\n",
    "    # returns all the videos of a particular Youtube\n",
    "    # channel, and returns data for each video (title, description,\n",
    "    # comments, date, statistics, etc.)\n",
    "    global current_key\n",
    "    print(\"Retrieving data for \" + channel_id)\n",
    "    res = None\n",
    "    while res == None:\n",
    "        try:\n",
    "            res = yt.channels().list(id=channel_id,part='contentDetails').execute()\n",
    "        except errors.HttpError as error:\n",
    "            if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                print(\"Disabled comments: %s\" % error)\n",
    "                pass\n",
    "            elif error.resp.status == 403:\n",
    "                print(\"Error1: %s\" % error)\n",
    "                yt = door_spin()\n",
    "                if yt == None:\n",
    "                    print(\"Breaking\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error2: %s\" % error)\n",
    "                pass\n",
    "    items = res['items']\n",
    "    playlist_id = items[0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    next_page = None\n",
    "    page = 0\n",
    "    pages = 20\n",
    "    videos = []\n",
    "    while True:\n",
    "        print(\"Current API index: \" + str(current_key))\n",
    "        try:\n",
    "            result = yt.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,\n",
    "                                             pageToken = next_page).execute()\n",
    "            for res in result['items']:\n",
    "                video_id = res['snippet']['resourceId']['videoId']\n",
    "                video_title = res['snippet']['title']\n",
    "                video_publish_date = res['snippet']['publishedAt']\n",
    "                video_description = res['snippet']['description']\n",
    "                if random.random() > 0.20:\n",
    "                    try:\n",
    "                        authors, stats = get_video_comments(video_id, yt)\n",
    "                        videos.append([video_title, video_publish_date, video_description, stats, authors])\n",
    "                    except errors.HttpError as error:\n",
    "                        if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                            print(\"Disabled comments: %s\" % error)\n",
    "                            pass\n",
    "                        elif error.resp.status == 403:\n",
    "                            print(\"Error1: %s\" % error)\n",
    "                            yt = door_spin()\n",
    "                            if yt == None:\n",
    "                                print(\"Breaking\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(\"Error2: %s\" % error)\n",
    "                            pass\n",
    "            if random.random()>0.80:\n",
    "                next_page = yt.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,\n",
    "                pageToken = result.get('nextPageToken')).execute().get('nextPageToken')\n",
    "            else:\n",
    "                next_page = result.get('nextPageToken')\n",
    "                page += 1\n",
    "                print(\"Current page: \" + str(page))\n",
    "        except errors.HttpError as error:\n",
    "            if error_reason(error).split(\" \")[-1] == \"commentsDisabled\":\n",
    "                print(\"Disabled comments: %s\" % error)\n",
    "                pass\n",
    "            elif error.status == 403:\n",
    "                print(\"Error1: %s\" % error)\n",
    "                yt = door_spin()\n",
    "                if yt == None:\n",
    "                    print(\"Breaking\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"Error2: %s\" % error)\n",
    "                pass\n",
    "        if next_page == None or page > pages:\n",
    "            break\n",
    "        print(str(len(videos)) + \" videos retrieved from the API so far\")\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some channels might appear as a \"user\" instead of a \"channel,\" so we convert those by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_id = yt.channels().list(part=\"id\",forUsername='CNN').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_id = yt.channels().list(part=\"id\",forUsername='FoxNewsChannel').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_times_id = yt.channels().list(part=\"id\",forUsername='TheNewYorkTimes').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_id = yt.channels().list(part=\"id\",forUsername='msnbcleanforward').execute()['items'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "breitbart_id = \"UCmgnsaQIK1IR808Ebde-ssA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = channel_videos(cnn_id, yt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fox_data = channel_videos(fox_id, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_times_data = channel_videos(ny_times_id, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msnbc_data = channel_videos(msnbc_id, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data for UCmgnsaQIK1IR808Ebde-ssA\n",
      "Current API index: 5\n",
      "Current page: 1\n",
      "43 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 2\n",
      "84 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 3\n",
      "127 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 4\n",
      "167 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "210 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 5\n",
      "245 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 6\n",
      "279 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 7\n",
      "322 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 8\n",
      "358 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "397 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "443 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 9\n",
      "482 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 10\n",
      "519 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Disabled comments: <HttpError 403 when requesting https://www.googleapis.com/youtube/v3/commentThreads?videoId=T1hRkDaID0M&part=snippet%2Creplies&maxResults=100&key=AIzaSyCCCQsP3NGY9rGvOBI5unIvEy4OsAC41tc&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\">\n",
      "Current page: 11\n",
      "553 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 12\n",
      "588 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 13\n",
      "632 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 14\n",
      "673 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "710 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 15\n",
      "748 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 16\n",
      "787 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 17\n",
      "823 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 18\n",
      "861 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Current page: 19\n",
      "900 videos retrieved from the API so far\n",
      "Current API index: 5\n",
      "Error1: <HttpError 403 when requesting https://www.googleapis.com/youtube/v3/commentThreads?videoId=uT5yUTqvPrk&part=snippet%2Creplies&maxResults=100&key=AIzaSyCCCQsP3NGY9rGvOBI5unIvEy4OsAC41tc&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\">\n",
      "Current page: 20\n",
      "938 videos retrieved from the API so far\n",
      "Current API index: 6\n",
      "Current page: 21\n"
     ]
    }
   ],
   "source": [
    "breitbart_data = channel_videos(breitbart_id, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cnn_data) + len(breitbart_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Caching and Processing \n",
    "We will now process our data and organize it so that \n",
    "it may be used for analysis. We will also cache our data\n",
    "at this point, since Google restricts the number of requests\n",
    "we can make with the API and we want to make the most of\n",
    "every request we are allowed. We will include an unpackaging \n",
    "method so that we can retrieve the data from our text files\n",
    "in the same format as it is returned in the channel_videos\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_channel_data(data, channel):\n",
    "    channel_file = open(channel+\".txt\",\"w\")\n",
    "    new_line = \"\\n\\n\\n\"\n",
    "    for video in data:\n",
    "        # video_title, video_publish_date, video_description, stats, authors\n",
    "        video_title, video_publish_date, video_description, stats, authors = video\n",
    "        vid_doc = video_title+\"\\t\"+video_publish_date+\"\\t\"+\\\n",
    "        video_description+\"\\t\"+str(stats)+\"\\t\"\n",
    "        channel_file.write(vid_doc)\n",
    "        for author in authors:\n",
    "            for comment in authors[author]:\n",
    "                text = comment[0]\n",
    "                likes = comment[1]\n",
    "                comment_doc = author+\"x<>{i}<>x\"+\\\n",
    "                text+\"<>x<>x<>\"+str(likes)+\"<>x<>x<>\"\n",
    "                channel_file.write(comment_doc)\n",
    "                channel_file.write(\"<><>cachecomment<><>\")\n",
    "        channel_file.write(\"<><>cachevideo<><>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will include an unpackaging \n",
    "method so that we can retrieve the data from our text files\n",
    "in the same format as it is returned in the channel_videos\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_cached_data(channel):\n",
    "    channel_file = open(channel,\"r\")\n",
    "    videos = channel_file.read().lstrip().split(\"<><>cachevideo<><>\")\n",
    "    video_list = []\n",
    "    for video in videos:\n",
    "        try:\n",
    "            d=video.split(\"\\t\")\n",
    "            video_title = d[0]\n",
    "            video_date = d[1]\n",
    "            video_description = d[2]\n",
    "            video_stats = ast.literal_eval(d[3])\n",
    "        except:\n",
    "            continue\n",
    "        authors = {}\n",
    "        def exists(key):\n",
    "            return key in authors\n",
    "        for comment_data in d[4].split(\"<><>cachecomment<><>\"):\n",
    "            comment_data = comment_data.strip()\n",
    "            if comment_data == \"\":\n",
    "                pass\n",
    "            else:\n",
    "                data = comment_data.split(\"x<>{i}<>x\")\n",
    "                author = data[0]\n",
    "                comment = data[1].split(\"<>x<>x<>\")[0]\n",
    "                likes = data[1].split(\"<>x<>x<>\")[0]\n",
    "                if exists(author):\n",
    "                    authors[author].append([comment,likes])\n",
    "                else:\n",
    "                    authors[author] = []\n",
    "                    authors[author].append([comment,likes])\n",
    "        video_list.append([video_title,video_date,video_description,video_stats,authors])\n",
    "    return video_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(cnn_data, \"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(fox_data, \"Fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_channel_data(breitbart_data, \"Breitbart News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_cnn_data=unpack_cached_data(\"CNN.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unpacked_cnn_data) + len(unpacked_cnn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data cached, we can now begin to look at some basic trends. We'll do some analysis on the video statistics and plot the like/dislike ratios for videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the necessary libraries to do plotting and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import datetime\n",
    "from matplotlib.dates import WeekdayLocator\n",
    "from matplotlib.dates import DayLocator\n",
    "from matplotlib.dates import (YEARLY, DateFormatter,rrulewrapper, RRuleLocator, drange)\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write some helper functions to take care of the details. First, a function to convert the raw dates from our list of videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_date(date):\n",
    "    date_raw = date.split(\"T\")[0].split(\"-\")\n",
    "    year = int(date_raw[0])\n",
    "    month = int(date_raw[1])\n",
    "    day = int(date_raw[2])\n",
    "    return year, month, day\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write a function to process the statistics for each video and return the number of views, likes, dislikes, and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_stats(video):\n",
    "    title, publish_date, description, stats, authors = video\n",
    "    year, month, day = convert_raw_date(publish_date)\n",
    "    date = datetime.date(year,month,day)\n",
    "    views = stats['viewCount']\n",
    "    likes = stats['likeCount']\n",
    "    dislikes = stats['dislikeCount']\n",
    "    num_comments = stats['commentCount']\n",
    "    return title, date, description, int(views), int(likes), int(dislikes), int(num_comments), authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our processing function to build up lists of different values that we can plot against the publishing dates of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(videos):\n",
    "    like_nums = []\n",
    "    dislike_nums = []\n",
    "    comment_nums = []\n",
    "    like_dislike_ratios = []\n",
    "    dates = []\n",
    "    titles = []\n",
    "    comments = []\n",
    "    view_nums = []\n",
    "    interactions = []\n",
    "    descriptions = []\n",
    "    for video in videos:\n",
    "        title, date, description, views, likes, dislikes, num_comments, authors=process_video_stats(video)\n",
    "        like_nums.append(likes)\n",
    "        dislike_nums.append(dislikes)\n",
    "        like_dislike_ratios.append(dislikes/(likes+dislikes))\n",
    "        comment_nums.append(num_comments)\n",
    "        dates.append(date)\n",
    "        titles.append(title)\n",
    "        comments.append(authors)\n",
    "        view_nums.append(views)\n",
    "        interactions.append(num_comments+likes+dislikes)\n",
    "        descriptions.append(description)\n",
    "    return like_nums, dislike_nums, comment_nums, like_dislike_ratios,\\\n",
    "    dates, titles, comments, view_nums, descriptions, interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cnn_data = process_videos(cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_fox_data = process_videos(fox_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0, f1, f2,\\\n",
    "f3, f4, f5, fox_comment_dictionaries,\\\n",
    "f7, f8, f9 = processed_fox_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_authors(comment_dictionaries):\n",
    "    unique_authors = []\n",
    "    for video in comments:\n",
    "        for author in video:\n",
    "            if author in unique_authors:\n",
    "                continue\n",
    "            else:\n",
    "                unique_authors.append(author)\n",
    "    write_list_to_file(unique_authors, \"authors.txt\")\n",
    "    return unique_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(y, title):\n",
    "    file=open(title,\"w\")\n",
    "    for x in y:\n",
    "        file.write(author + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now write a plotting function to plot based on the metrics we've produced from our processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def plot_data(data, dates, title):\n",
    "    # tick every 5th easter\n",
    "    mpl.rcParams['figure.dpi'] = 150\n",
    "    loc = DayLocator(interval=10)\n",
    "    formatter = DateFormatter('%Y-%m-%d')\n",
    "    fig, ax = plt.subplots()\n",
    "    y_mean = statistics.mean(data)\n",
    "    st=\"{:.3f}\".format(y_mean)\n",
    "    #print(st)\n",
    "    plt.plot_date(np.array(dates), np.array(data))\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    ax.xaxis.set_tick_params(rotation=30, labelsize=10)\n",
    "    plt.axhline(y=y_mean, color='r', linestyle='-',label='Mean Average: {}'.format(float(st)))\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(20,5))\n",
    "    style.use('fivethirtyeight')\n",
    "    plt.show()\n",
    "def plot_datas(data, data2, title):\n",
    "    mpl.rcParams['figure.dpi'] = 150\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "    style.use('fivethirtyeight')\n",
    "    plt.scatter(data,data2,s=2)\n",
    "    z = numpy.polyfit(data, data2, 1)\n",
    "    p = numpy.poly1d(z)\n",
    "    #plt.plot(data,p(data),\"r--\")\n",
    "    # the line equation:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to plot video statistics over time from any given channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[0],processed_cnn_data[4], \"CNN Video Likes\")\n",
    "plot_data(processed_fox_data[0],processed_fox_data[4], \"FOX News Video Likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[1],processed_cnn_data[4], \"CNN Video Dislikes\")\n",
    "plot_data(processed_fox_data[1],processed_fox_data[4], \"FOX News Video Dislikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[3],processed_cnn_data[4], \"CNN Video Dislikes/Total\")\n",
    "plot_data(processed_fox_data[3],processed_fox_data[4], \"FOX News Video Dislikes/Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[7],processed_cnn_data[4], \"CNN Video Views\")\n",
    "plot_data(processed_fox_data[7],processed_fox_data[4], \"FOX News Video Views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[2],processed_cnn_data[4], \"CNN Video Comments\")\n",
    "plot_data(processed_fox_data[2],processed_fox_data[4], \"FOX News Video Comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(processed_cnn_data[9],processed_cnn_data[4], \"CNN Video Interactions\")\n",
    "plot_data(processed_fox_data[9],processed_fox_data[4], \"FOX News Video Interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to do a broad sentiment analysis for each channel's videos, we can write some functions to help us do that. We'll loop through our processed data and retrive the comments. For each video, we'll find the mean average positive, negative, and compound sentiment scores by aggregating these scores from each comment in the video. We'll also create an overloaded method for analyzing sentiments towards a specific keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def analyze(comment):\n",
    "    score = analyzer.polarity_scores(comment)\n",
    "    return score[\"compound\"], score[\"pos\"], score[\"neg\"]\n",
    "def analyze_channel(data):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    compound_scores = []\n",
    "    like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates,\\\n",
    "    titles, comments, view_nums, description, interactions = data\n",
    "    for author_dict in comments: # video comments\n",
    "        vid_pos = []\n",
    "        vid_neg = []\n",
    "        vid_comp = []\n",
    "        for author in author_dict: # all the authors in the comment section\n",
    "            for comment in author_dict[author]: # their comments\n",
    "                comp, pos, neg = analyze(comment[0])\n",
    "                vid_pos.append(pos)\n",
    "                vid_neg.append(neg)\n",
    "                vid_comp.append(comp)\n",
    "        pos = statistics.mean(vid_pos)\n",
    "        neg = statistics.mean(vid_neg)\n",
    "        comp = statistics.mean(vid_comp)\n",
    "        pos_scores.append(pos)\n",
    "        neg_scores.append(neg)\n",
    "        compound_scores.append(comp)\n",
    "    return pos_scores,neg_scores,compound_scores\n",
    "def analyze_channel_by_keyword(data, keyword):\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    compound_scores = []\n",
    "    dates_by_keyword = []\n",
    "    like_nums, dislike_nums, comment_nums, like_dislike_ratios, dates,\\\n",
    "    titles, comments, view_nums, descriptions, interactions = data\n",
    "    index = 0\n",
    "    for author_dict in comments: # video comments\n",
    "        if keyword in titles[index] or keyword in descriptions[index]:\n",
    "            vid_pos = []\n",
    "            vid_neg = []\n",
    "            vid_comp = []\n",
    "            for author in author_dict: # all the authors in the comment section\n",
    "                for comment in author_dict[author]: # their comments\n",
    "                    comp, pos, neg = analyze(comment[0])\n",
    "                    vid_pos.append(pos)\n",
    "                    vid_neg.append(neg)\n",
    "                    vid_comp.append(comp)\n",
    "            pos = statistics.mean(vid_pos)\n",
    "            neg = statistics.mean(vid_neg)\n",
    "            comp = statistics.mean(vid_comp)\n",
    "            pos_scores.append(pos)\n",
    "            neg_scores.append(neg)\n",
    "            compound_scores.append(comp)\n",
    "            dates_by_keyword.append(dates[index])\n",
    "        index+=1\n",
    "    return pos_scores,neg_scores,compound_scores,dates_by_keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll retrieve the sentiment data for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pos, cnn_neg, cnn_comp, cnn_dates = analyze_channel_by_keyword(processed_cnn_data, \"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_pos, fox_neg, fox_comp, fox_dates = analyze_channel_by_keyword(processed_fox_data, \"Trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll take the sentiment data we got from our channel analysis with a keyword parameter of \"Trump\" to see how CNN viewers and Fox viewers tend to speak when the President is mentioned in or is the subject of one of their Youtube videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_pos,cnn_dates, \"CNN Viewer Sentiments (Trump, positive score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data(fox_comp,fox_dates, \"Fox Viewer Sentiments (Trump, compound score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_pos,cnn_dates,\"CNN Viewer Sentiments (Trump, positive score)\")\n",
    "plot_data(fox_pos,fox_dates,\"Fox Viewer Sentiments (Trump, positive score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(cnn_neg,cnn_dates,\"CNN Viewer Sentiments (Trump, negative score)\")\n",
    "plot_data(fox_neg,fox_dates,\"Fox Viewer Sentiments (Trump, negative score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_scores,neg_scores,compound_scores = analyze_channel(processed_cnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[3],pos_scores, \"Dislike Ratio Against Positive Sentiment Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[3],neg_scores, \"Dislike Ratio Against Negative Sentiment Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[3],compound_scores, \"Dislike Ratio Against Compound Sentiment Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datas(processed_cnn_data[9],compound_scores, \"Interactions vs. Compound Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll tokenize and stem the comments we've downloaded to create a bag of words."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
